use std::{collections::HashMap, sync::Arc, time::Duration};

use futures::{StreamExt, stream::FuturesUnordered};
use iodine_common::{
    command::CommandRouter,
    error::Error,
    pipeline::PipelineRunStatus,
    resource_manager::{
        ExecutionContext, ExecutionContextKind, ExecutionManager, LocalProcessExecutionContext,
        ProvisionedWorkerDetails, WorkerRequest,
    },
    state::DatabaseTrait,
    task::{TaskDefinition, TaskStatus},
};
use tokio::{
    sync::{mpsc, oneshot},
    task::{JoinError, JoinHandle},
    time::Instant,
};
use tracing::{error, info, warn};
use uuid::Uuid;

use super::{
    ActiveWorkerInfo, LauncherCommand, LauncherConfig, WorkerResult,
    exec_mgr_registry::ExecutionManagerRegistry, execution_graph::PipelineExecutionGraph,
};

pub struct Launcher {
    /// Unique ID for this launcher.
    pub id: Uuid,

    /// Launcher configuration.
    pub config: LauncherConfig,

    /// Command router for dispatching commands to handlers.
    pub command_router: Arc<CommandRouter>,

    /// State manager for accessing the database.
    pub state_manager: Arc<dyn DatabaseTrait>,

    /// Execution manager registry for accessing the appropriate managers.
    pub exec_manager_registry: Arc<ExecutionManagerRegistry>,

    /// Parent coordinator ID.
    pub(super) coordinator_id: Uuid,

    // Pipeline execution state
    /// The ID of the current pipeline definition being executed.
    pub(super) current_pipeline_definition_id: Option<Uuid>,

    /// The ID of the current pipeline run being executed.
    pub(super) current_pipeline_run_id: Option<Uuid>,

    pub(super) execution_graph: Option<PipelineExecutionGraph>,

    /// `TaskDefinition` ID -> Current `TaskStatus`
    pub(super) task_states: HashMap<Uuid, TaskStatus>,

    /// `TaskDefinition` ID -> Current task attempt
    pub(super) task_attempts: HashMap<Uuid, u32>,

    // Maps assigned_worker_id (UUID generated by Launcher for each attempt) to its info
    pub(super) active_workers: HashMap<Uuid, ActiveWorkerInfo>,

    worker_results_tx: mpsc::Sender<WorkerResult>,

    // this gets taken by run_loop
    worker_results_rx: Option<mpsc::Receiver<WorkerResult>>,

    /// Manages JoinHandles of the tasks that poll ExecutionManagers for worker status
    pub(super) polling_monitor_tasks: FuturesUnordered<JoinHandle<Result<(Uuid, String), Error>>>,
}

impl Launcher {
    pub async fn new(
        id: Uuid,
        coordinator_id: Uuid,
        config: LauncherConfig,
        command_router: Arc<CommandRouter>,
        state_manager: Arc<dyn DatabaseTrait>,
        exec_manager_registry: Arc<ExecutionManagerRegistry>,
    ) -> Result<Self, Error> {
        let worker_buf_size = config.worker_channel_buffer_size.unwrap_or(100);

        let (worker_results_tx, worker_results_rx) = mpsc::channel(worker_buf_size);

        Ok(Self {
            id,
            config,
            command_router,
            state_manager,
            exec_manager_registry,
            coordinator_id,
            current_pipeline_definition_id: None,
            current_pipeline_run_id: None,
            execution_graph: None,
            task_states: HashMap::new(),
            task_attempts: HashMap::new(),
            active_workers: HashMap::new(),
            worker_results_tx,
            worker_results_rx: Some(worker_results_rx),
            polling_monitor_tasks: FuturesUnordered::new(),
        })
    }

    pub async fn run_loop(&mut self) -> Result<(), Error> {
        let mut command_rx = self
            .command_router
            .subscribe::<LauncherCommand>(Some(self.id))
            .await?;

        let mut worker_results_rx = self.worker_results_rx.take().ok_or_else(|| {
            Error::Internal("Worker results receiver was already taken".to_string())
        })?;

        loop {
            tokio::select! {
                biased;

                Some(command) = command_rx.recv() => {
                    if let Err(e) = self.handle_launcher_command(command).await {
                        error!("Launcher [{}]: Error processing command: {}", self.id, e);

                        if matches!(e, Error::TerminateSignal) {
                            return Ok(()); // Exit run_loop after Terminate processed
                        }

                    }
                },
                Some(worker_result) = worker_results_rx.recv() => {
                    if let Err(e) = self.handle_worker_result(worker_result).await {
                        error!("Launcher [{}]: Error handling worker result: {:?}", self.id, e);
                        if let Some(run_id) = self.current_pipeline_run_id {
                            self.finalize_pipeline(
                                run_id,
                                PipelineRunStatus::Failed,
                                Some(format!("Critical error handling worker result: {}", e)),
                            ).await.ok();
                        }
                    }
                },
                Some(monitor_outcome_res) = self.polling_monitor_tasks.next(), if !self.polling_monitor_tasks.is_empty() => {
                    self.handle_monitoring_task_completion(monitor_outcome_res).await;
                },
                _ = tokio::time::sleep(self.config.scheduler_tick_interval.unwrap_or(Duration::from_secs(2))), if self.current_pipeline_run_id.is_some() => {
                    if let Err(e) = self.schedule_and_launch_tasks().await {
                        error!("Launcher [{}]: Error during periodic task scheduling: {:?}", self.id, e);
                    }
                },
                else => {
                    info!("Launcher [{}]: All event sources seem closed or idle. Awaiting Terminate command.", self.id);
                    // If truly idle and command_rx also closed (implicit in select! else), then break.
                    // However, command_rx typically stays open unless Coordinator closes its side.
                    // This branch is more for the case where all select arms are permanently finished.
                    // A long sleep here or reliance on Terminate command is typical.

                    // Prevent busy looping if in an unexpected state
                    tokio::time::sleep(Duration::from_secs(5)).await;
                }

            }
        }
    }

    async fn handle_launcher_command(&mut self, command: LauncherCommand) -> Result<(), Error> {
        match command {
            LauncherCommand::ExecutePipeline {
                pipeline_definition,
            } => {
                if self.current_pipeline_run_id.is_some() {
                    let msg = format!(
                        "Launcher [{}]: Received ExecutePipeline while pipeline run {:?} is already active.",
                        self.id, self.current_pipeline_run_id
                    );
                    error!("{}", msg);
                    return Err(Error::Conflict(msg));
                }

                self.initialize_pipeline_state(pipeline_definition)
                    .await?;

                self.schedule_and_launch_tasks().await?;
            }
            LauncherCommand::Terminate { ack_chan } => {
                let teardown_res = self.teardown_launcher().await;
                let ack_ok = teardown_res.is_ok();
                if ack_chan.send(ack_ok).is_err() {
                    error!("Launcher [{}]: Failed to send Terminate ACK.", self.id);
                }

                if !ack_ok {
                    error!(
                        "Launcher [{}]: Teardown errors: {:?}",
                        self.id,
                        teardown_res.err()
                    );
                }
                return Err(Error::TerminateSignal);
            }
        }

        Ok(())
    }

    pub(super) async fn launch_task_worker(
        &mut self,
        task_def: &TaskDefinition,
        pipeline_run_id: Uuid,
        attempt: u32,
    ) -> Result<(), Error> {
        let assigned_worker_id = Uuid::new_v4();

        // FIXME(thegenem0):
        // This should come from a field like `task_def.execution_details`
        let exec_ctx = ExecutionContext::LocalProcess(LocalProcessExecutionContext {
            entry_point: vec!["sh".to_string()],
            args: vec![
                "-c".to_string(),
                "mkdir -p /tmp/iodine-worker-data && echo 'Hello, world!' > /tmp/iodine-worker-data/hello.txt".to_string()
            ],
            env_vars: HashMap::new(),
            exec_timeout: None,
        });

        let exec_ctx_kind = ExecutionContextKind::from(&exec_ctx);

        let manager = self
            .exec_manager_registry
            .get_manager(exec_ctx_kind)
            .ok_or_else(|| {
                Error::Internal(format!(
                    "Failed to find ExecutionManager for context kind {:?}",
                    exec_ctx_kind
                ))
            })?;

        let worker_request = WorkerRequest {
            pipeline_id: self
                .current_pipeline_definition_id
                .expect("Pipeline Def ID must be set when launching worker"),
            run_id: pipeline_run_id,
            worker_id: assigned_worker_id,
            execution_context: exec_ctx,
        };

        info!(
            "Launcher [{}]: Task '{}' (ID: {}), attempt {}: Sending request to EM [{}] for worker_id {}.",
            self.id,
            task_def.name,
            task_def.id,
            attempt,
            manager.manager_id(),
            assigned_worker_id
        );

        self.task_states.insert(task_def.id, TaskStatus::Queued);
        self.task_attempts.insert(task_def.id, attempt);

        // FIXME(thegenem0):
        // Add this state update method
        //  self.state_manager.update_task_run_status(
        //     pipeline_run_id, task_id, attempt_number, TaskStatus::Submitted,
        //     Some(Utc::now()), None, Some(format!("Submitted to EM {}, launcher_worker_id {}", manager.manager_id(), launcher_worker_id))
        // ).await?;

        let provisioned_details = match manager.provision_and_start_execution(&worker_request).await
        {
            Ok(details) => {
                if details.worker_id != assigned_worker_id {
                    return Err(Error::Internal(format!(
                        "ExecutionManager {} returned mismatched worker_id for task {}. Expected {}, got {}.",
                        manager.manager_id(),
                        task_def.id,
                        assigned_worker_id,
                        details.worker_id
                    )));
                }

                details
            }
            Err(e) => {
                error!(
                    "Launcher [{}]: EM failed to provision/start worker {} for task {}: {:?}",
                    self.id, assigned_worker_id, task_def.id, e
                );
                return Err(e);
            }
        };

        let (monitor_cancel_tx, monitor_cancel_rx) = oneshot::channel::<()>();

        let active_worker_info = ActiveWorkerInfo {
            task_id: task_def.id,
            attempt,
            provisioned_details: provisioned_details.clone(),
            monitor_cancel_tx: Some(monitor_cancel_tx),
            task_name: task_def.name.clone(),
        };

        self.active_workers
            .insert(assigned_worker_id, active_worker_info);

        let task_def_id = task_def.id;
        let task_def_name = task_def.name.clone();
        let em_clone = Arc::clone(&manager);
        let results_tx_clone = self.worker_results_tx.clone();
        let polling_interval = self
            .config
            .worker_polling_interval
            .unwrap_or(Duration::from_secs(10));

        let monitor_handle: JoinHandle<Result<(Uuid, String), Error>> = tokio::spawn(async move {
            Self::poll_worker_status_loop(
                assigned_worker_id,
                task_def_id,
                attempt,
                task_def_name,
                em_clone,
                provisioned_details,
                results_tx_clone,
                monitor_cancel_rx,
                polling_interval,
            )
            .await
        });

        self.polling_monitor_tasks.push(monitor_handle);

        Ok(())
    }

    #[allow(clippy::too_many_arguments)]
    async fn poll_worker_status_loop(
        assigned_worker_id: Uuid,
        task_id: Uuid,
        attempt: u32,
        task_name: String,
        em: Arc<dyn ExecutionManager>,
        details: ProvisionedWorkerDetails,
        results_tx: mpsc::Sender<WorkerResult>,
        mut cancel_rx: oneshot::Receiver<()>,
        polling_interval: Duration,
    ) -> Result<(Uuid, String), Error> {
        info!(
            "[PollingMonitor for '{}' ({})] Started. Polling EM [{}] every {:?}.",
            task_name,
            assigned_worker_id,
            em.manager_id(),
            polling_interval
        );

        #[allow(unused_assignments)]
        let mut final_task_status: Option<TaskStatus> = None;

        #[allow(unused_assignments)]
        let mut final_message: Option<String> = None;

        loop {
            tokio::select! {
                biased;

                _ = &mut cancel_rx => {
                    info!("[PollingMonitor for '{}' ({})] Cancellation signal received from Launcher.", task_name, assigned_worker_id);

                    if let Err(e) = em.cancel_execution(&details).await {
                        error!("[PollingMonitor for '{}' ({})] EM failed to process cancel_execution: {}. Marking as Cancelled anyway.", task_name, assigned_worker_id, e);
                    } else {
                        info!("[PollingMonitor for '{}' ({})] EM cancel_execution succeeded.", task_name, assigned_worker_id);
                    }

                    final_task_status = Some(TaskStatus::Cancelled);
                    final_message = Some("Cancelled by Launcher request.".to_string());
                    break;
                }

                _ = tokio::time::sleep(polling_interval) => {
                        match em.get_execution_status(&details).await {
                            Ok(em_status) => {
                                let (current_task_status, msg, is_terminal) = Self::map_em_status_to_task_status(em_status.clone());
                                info!("[PollingMonitor for '{}' ({})] Polled EM status: {:?} -> TaskStatus: {:?}", task_name, assigned_worker_id, em_status, current_task_status);

                                if is_terminal {
                                    final_task_status = Some(current_task_status);
                                    final_message = msg;
                                    break;
                                } else if current_task_status != TaskStatus::Running {
                                    // Optionally could send intermediate "Running" status?
                                    // only final status is critical for DAG progression, but
                                    // might be nice to have?
                                }
                            }
                            Err(e) => {
                                error!("[PollingMonitor for '{}' ({})] Error polling EM status: {}. Marking as failed.", task_name, assigned_worker_id, e);
                                final_task_status = Some(TaskStatus::Failed);
                                final_message = Some(format!("Error polling EM: {}", e));
                                break;
                            }
                        }
                    }
            } // select!
        } // loop

        let result_to_send = WorkerResult {
            assigned_worker_id,
            task_id,
            attempt,
            final_status: final_task_status.unwrap_or(TaskStatus::Failed),
            message: final_message,
        };

        if results_tx.send(result_to_send).await.is_err() {
            error!(
                "[PollingMonitor for '{}' ({})] Failed to send its final result to Launcher (channel closed).",
                task_name, assigned_worker_id
            );
        }

        if let Err(teardown_err) = em.teardown_worker(&details).await {
            error!(
                "[PollingMonitor for '{}' ({})] Error during EM teardown_worker: {}",
                task_name, assigned_worker_id, teardown_err
            );
        }

        info!(
            "[PollingMonitor for '{}' ({})] Exiting.",
            task_name, assigned_worker_id
        );

        Ok((assigned_worker_id, task_name))
    }

    async fn handle_worker_result(&mut self, result: WorkerResult) -> Result<(), Error> {
        info!(
            "Launcher [{}]: Handling WorkerResult - launcher_worker_id: {}, task_id: {}, attempt: {}, status: {:?}, msg: '{}'",
            self.id,
            result.assigned_worker_id,
            result.task_id,
            result.attempt,
            result.final_status,
            result.message.as_deref().unwrap_or("")
        );

        // Remove from active_workers.
        // The monitor task is responsible for EM.teardown_worker.
        if self
            .active_workers
            .remove(&result.assigned_worker_id)
            .is_none()
        {
            error!(
                "Launcher [{}]: Warning - Worker {} was not found in active_workers when its result arrived.",
                self.id, result.assigned_worker_id
            );
        }

        self.task_states.insert(result.task_id, result.final_status);

        self.task_attempts.insert(result.task_id, result.attempt);

        let _pipeline_run_id = self.current_pipeline_run_id.ok_or_else(|| {
            Error::Internal(format!(
                "current_pipeline_run_id missing for task {} result",
                result.task_id
            ))
        })?;

        // FIXME(thegenem0):
        // Add this state update method
        //  self.state_manager.update_task_run_status(
        //     pipeline_run_id, result.task_id, result.attempt,
        //     result.final_status.clone(),
        //     None, Some(Utc::now()),
        //     // TODO(thegenem0): Get actual start/end times from EM (via WorkerResult/ProvisionedWorkerDetails)
        //     result.message,
        // ).await?;

        if result.final_status == TaskStatus::Failed {
            let _task_def = self
                .execution_graph
                .as_ref()
                .and_then(|g| g.get_task_definition(result.task_id))
                .ok_or_else(|| {
                    Error::Internal(format!(
                        "TaskDef not found for failed task {}",
                        result.task_id
                    ))
                })?;

            let max_retries = 3; // FIXME(thegenem0): Get from task_def

            if result.attempt >= max_retries {
                info!(
                    "Launcher [{}]: Task {} (launcher_worker_id {}) failed on attempt {}, will retry (max retries {}).",
                    self.id, result.task_id, result.assigned_worker_id, result.attempt, max_retries
                );

                // reset for next scheduling cycle
                self.task_states
                    .insert(result.task_id, TaskStatus::Retrying);
                // The attempt count in self.task_attempts (number of attempts *made*) is already correct.
                // The next call to schedule_and_launch_tasks will use `self.task_attempts.get() + 1`.
            } else {
                info!(
                    "Launcher [{}]: Task {} (launcher_worker_id {}) failed on final attempt {} (max retries {}). No more retries.",
                    self.id, result.task_id, result.assigned_worker_id, result.attempt, max_retries
                );
                // Status is already `Failed`. No state change needed here for retries.
            }
        }

        self.schedule_and_launch_tasks().await
    }

    async fn handle_monitoring_task_completion(
        &mut self,
        monitor_outcome_res: Result<Result<(Uuid, String), Error>, JoinError>,
    ) {
        let (worker_id_opt, task_name_opt, error_str_opt) = match monitor_outcome_res {
            Ok(Ok((assigned_worker_id, task_name))) => {
                (Some(assigned_worker_id), Some(task_name), None)
            }
            Ok(Err(e)) => (
                // Error from the monitor task itself
                None,
                None,
                Some(format!("Monitor task returned error: {}", e)),
            ),
            Err(join_err) => (
                // Monitor task panicked
                None,
                None,
                Some(format!("Monitor task panicked: {}", join_err)),
            ),
        };

        if let Some(worker_id) = worker_id_opt {
            info!(
                "Launcher [{}]: Polling monitor for worker {} (task '{}') has fully exited.",
                self.id,
                worker_id,
                task_name_opt.unwrap_or_default()
            );

            if self.active_workers.remove(&worker_id).is_some() {
                error!(
                    "Launcher [{}]: CRITICAL - Worker {} was still in active_workers when its monitor JoinHandle completed. WorkerResult message likely lost or monitor task errored before sending.",
                    self.id, worker_id
                );
                // FIXME(thegenem0): Need a robust way to mark the associated pipeline task as errored here.
                // This requires mapping worker_id back to task_id if not available from join_handle return.
                // For now, this state means we might have a zombie task status.
            }
        } else if let Some(err_msg) = error_str_opt {
            error!(
                "Launcher [{}]: A polling monitor task exited with an issue: {}. Unable to map to specific worker ID from JoinHandle outcome directly.",
                self.id, err_msg
            );
            // This is a tricky state. Might need to iterate all active_workers and check their JoinHandles if they were stored,
            // or consider the pipeline unstable.
        }

        if self.current_pipeline_run_id.is_some() {
            if let Err(e) = self.schedule_and_launch_tasks().await {
                error!(
                    "Launcher [{}]: Error during task scheduling after monitor task completion: {}",
                    self.id, e
                );
            }
        }
    }

    pub(super) async fn finalize_pipeline(
        &mut self,
        run_id: Uuid,
        status: PipelineRunStatus,
        message: Option<String>,
    ) -> Result<(), Error> {
        let def_id = self.current_pipeline_definition_id.ok_or_else(|| {
            Error::Internal("Cannot finalize pipeline, definition ID missing.".to_string())
        })?;

        info!(
            "Launcher [{}]: Finalizing pipeline run {} (def {}) with status: {:?}, msg: {:?}",
            self.id,
            run_id,
            def_id,
            status,
            message.as_deref().unwrap_or("")
        );

        self.state_manager
            .update_pipeline_run_status(run_id, status, message)
            .await?;

        // TODO(thegenem0): Notify Coordinator via CommandRouter
        // Example:
        // let update_to_coord = CoordinatorCommand::PipelineStatusUpdate {
        //     pipeline_run_id: run_id,
        //     pipchrome://vivaldi-webui/startpage?section=Speed-dials&background-color=#1c1c1eeline_def_id: def_id,
        //     launcher_id: self.id,
        //     status,
        // };
        // if let Err(e) = self.command_router.dispatch(None, update_to_coord).await { // Assuming Coordinator is singleton
        //     eprintln!("Launcher [{}]: Failed to send pipeline completion update to Coordinator: {}", self.id, e);
        // }

        self.cleanup_current_pipeline_state();

        info!(
            "Launcher [{}]: Pipeline run {} state cleaned up.",
            self.id, run_id
        );
        Ok(())
    }

    async fn teardown_launcher(&mut self) -> Result<(), Error> {
        info!(
            "Launcher [{}]: Initiating teardown. Current pipeline run: {:?}",
            self.id, self.current_pipeline_run_id
        );

        let active_worker_ids_to_cancel: Vec<Uuid> = self.active_workers.keys().cloned().collect();
        if !active_worker_ids_to_cancel.is_empty() {
            info!(
                "Launcher [{}]: Sending cancel signals to {} active worker monitors...",
                self.id,
                active_worker_ids_to_cancel.len()
            );
        }

        for assigned_worker_id in &active_worker_ids_to_cancel {
            if let Some(worker_info) = self.active_workers.get_mut(assigned_worker_id) {
                if let Some(cancel_tx) = worker_info.monitor_cancel_tx.take() {
                    if cancel_tx.send(()).is_err() {
                        error!(
                            "Launcher [{}]: Failed to send cancel signal to monitor for worker {} (monitor task might have already exited).",
                            self.id, assigned_worker_id
                        );
                    }
                }
            }
        }

        info!(
            "Launcher [{}]: Waiting for {} monitoring tasks to complete their shutdown...",
            self.id,
            self.polling_monitor_tasks.len()
        );

        let teardown_timeout = Duration::from_secs(30);
        let mut tasks_to_await = std::mem::take(&mut self.polling_monitor_tasks);
        let start_teardown_await = Instant::now();

        while !tasks_to_await.is_empty() {
            if start_teardown_await.elapsed() > teardown_timeout {
                error!(
                    "Launcher [{}]: Timeout waiting for {} remaining monitoring tasks during teardown. Proceeding...",
                    self.id,
                    tasks_to_await.len()
                );

                tasks_to_await.iter().for_each(|jh| jh.abort());
                break;
            }

            match tokio::time::timeout(Duration::from_secs(1), tasks_to_await.next()).await {
                Ok(Some(monitor_outcome_res)) => match monitor_outcome_res {
                    Ok(Ok((worker_id, task_name))) => info!(
                        "Launcher [{}]: Monitor for {} ({}) completed during teardown.",
                        self.id, worker_id, task_name
                    ),
                    Ok(Err(e)) => error!(
                        "Launcher [{}]: Monitor task returned error during teardown: {:?}",
                        self.id, e
                    ),

                    Err(e) => error!(
                        "Launcher [{}]: Monitor task panicked during teardown: {:?}",
                        self.id, e
                    ),
                },
                Ok(None) => {
                    /* FuturesUnordered is empty */
                    break;
                }
                Err(_) => { /* Timeout for one task, loop again if total timeout not reached */ }
            }
        }

        self.active_workers.clear();

        if let Some(run_id) = self.current_pipeline_run_id {
            let is_already_terminal = self.execution_graph.as_ref().map_or_else(
                || true,
                |g| g.is_pipeline_complete(&self.task_states).is_some(),
            );

            if !is_already_terminal {
                warn!(
                    "Launcher [{}]: Marking pipeline run {} as Aborted due to launcher teardown.",
                    self.id, run_id
                );

                self.state_manager
                    .update_pipeline_run_status(
                        run_id,
                        PipelineRunStatus::Cancelled,
                        Some("Launcher performing teardown.".to_string()),
                    )
                    .await?;
            }
        }

        self.cleanup_current_pipeline_state();

        info!("Launcher [{}]: Teardown process complete.", self.id);

        Ok(())
    }
}
